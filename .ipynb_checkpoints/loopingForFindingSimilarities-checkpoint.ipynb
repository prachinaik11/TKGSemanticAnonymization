{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fg75sd_MN9fK",
    "outputId": "5da312ee-b8d6-43bf-d50a-c9f1d9f04489"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "h69OKgDSN9dA"
   },
   "outputs": [],
   "source": [
    "# cp /content/drive/MyDrive/dataset.csv /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9GKSI6JN9ag",
    "outputId": "2ee5cbfa-6abc-4e59-8023-c34eaef0eeda"
   },
   "outputs": [],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "722-DF5dUaoH"
   },
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers\n",
    "# !pip install scikit-learn-extra\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0tWjHDEhZ84R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachi/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# df=pd.read_csv(\"dataset.csv\")\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# df=pd.read_csv(\"dataset.csv\")\n",
    "df=pd.read_csv(\"output.tsv\", delimiter='\\t')\n",
    "# print(type(df.iloc[:, 0]))\n",
    "all_dict = df.set_index('sentences').to_dict()\n",
    "sentences_dict = all_dict.get('sensitiveAttr')\n",
    "\n",
    "sentences = list(sentences_dict.keys())\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbC6XbS1BI2q"
   },
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "import random\n",
    "from math import comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH7JZEw2-g6S"
   },
   "outputs": [],
   "source": [
    "# # all_z = [3,4,5,6,7,8,9,10]\n",
    "# # all_l = [2,3,4]\n",
    "\n",
    "# # all_z = [3]\n",
    "# # all_l = [2,3]\n",
    "# for l in all_l:\n",
    "#   for z in all_z:\n",
    "#     print(z,\" -- \",l)\n",
    "#     k = z\n",
    "#     # Use PCA to reduce the dimensionality to 2 for visualization\n",
    "#     pca = PCA(n_components=2)\n",
    "#     reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "#     # Use K-Medoids clustering\n",
    "#     num_clusters = int(len(sentences_dict)/k)\n",
    "\n",
    "#     kmedoids = KMedoids(n_clusters=num_clusters, random_state=42)\n",
    "#     cluster_labels = kmedoids.fit_predict(embeddings)\n",
    "\n",
    "#     # Visualize clusters\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     for i in range(num_clusters):\n",
    "#         cluster_points = reduced_embeddings[cluster_labels == i]\n",
    "#         plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}', alpha=0.7)\n",
    "\n",
    "#     plt.title('Visualization of Sentence Embeddings with Clusters (PCA)')\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Create groups of three from each cluster\n",
    "#     groups = [[] for _ in range(num_clusters)]\n",
    "#     for i, sentence in enumerate(sentences):\n",
    "#         cluster_index = cluster_labels[i]\n",
    "#         groups[cluster_index].append(sentence)\n",
    "\n",
    "#     # Display groups\n",
    "#     for i, group in enumerate(groups):\n",
    "#         print(f'Group {i + 1}:', group)\n",
    "#         print(f'Group {i + 1} size:', len(group))\n",
    "\n",
    "#     valid_groups = []\n",
    "#     invalid_groups = []\n",
    "#     all_unique_SA = set()\n",
    "\n",
    "#     for group in groups:\n",
    "#       print(len(group))\n",
    "#       unique_SA = set()\n",
    "#       for i in group:\n",
    "#         print(i)\n",
    "#         unique_SA.add(sentences_dict[i])\n",
    "#         all_unique_SA.add(sentences_dict[i])\n",
    "#       print(\"Unique l values : \",unique_SA)\n",
    "#       if len(unique_SA) >= l and len(group) >= k and len(group) <= (2*k-1):\n",
    "#         valid_groups.append(group)\n",
    "#       else:\n",
    "#         invalid_groups.append(group)\n",
    "#       print(\"------------------------------------------\")\n",
    "\n",
    "#     print(\"VALID GROUPS : \")\n",
    "#     for group in valid_groups:\n",
    "#       print(group)\n",
    "\n",
    "\n",
    "#     #added 14feb\n",
    "#     new_SE_groups = {}\n",
    "#     new_SA_groups = {}\n",
    "#     non_unique_SAs_per_group = []\n",
    "#     new_index = 0\n",
    "#     temp_group = []\n",
    "#     print(\"new_SE_groups: \",new_SE_groups)\n",
    "#     print(\"new_SE_groups: \",len(new_SE_groups))\n",
    "#     print(\"new_SA_groups: \",new_SA_groups)\n",
    "#     print(\"new_SA_groups: \",len(new_SA_groups))\n",
    "#     print(\"new_index: \",new_index)\n",
    "\n",
    "\n",
    "#     #added 14feb\n",
    "#     new_SE_groups = {}\n",
    "#     new_SA_groups = {}\n",
    "#     non_unique_SAs_per_group = []\n",
    "#     new_index = 0\n",
    "#     temp_group = []\n",
    "#     print(\"new_SE_groups: \",new_SE_groups)\n",
    "#     print(\"new_SE_groups: \",len(new_SE_groups))\n",
    "#     print(\"new_SA_groups: \",new_SA_groups)\n",
    "#     print(\"new_SA_groups: \",len(new_SA_groups))\n",
    "#     print(\"new_index: \",new_index)\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"INVALID GROUPS : \")\n",
    "#     invalid_to_valid_groups = {}\n",
    "#     valid_groups_with_count_of_nodes = {}\n",
    "#     fake_nodes = 0\n",
    "#     discarded_entries = 0\n",
    "\n",
    "#     countries_sum = 0\n",
    "#     ages_sum = 0\n",
    "#     alumniOf_sum = 0\n",
    "#     langs_sum = 0\n",
    "#     VtU_num = 0\n",
    "\n",
    "#     all_countries = set()\n",
    "#     all_ages = set()\n",
    "#     all_alumniOfs = set()\n",
    "#     all_langs = set()\n",
    "\n",
    "#     for group in groups:\n",
    "\n",
    "#       num_of_nodes_in_a_group = len(group)\n",
    "#       node_key = \"\"\n",
    "#       node_val = \"\"\n",
    "#       country_set = set()\n",
    "#       age_set = set()\n",
    "#       alumni_of_set = set()\n",
    "#       lang_spoken_set = set()\n",
    "#       unique_SA = set()\n",
    "#       non_unique_SAs_per_group = []   #added 14feb\n",
    "#       temp_group = []  #added 14feb\n",
    "#       temp_group = group   #added 14feb\n",
    "#       fake_nodes_in_a_group = 0\n",
    "\n",
    "#       for i in group:\n",
    "#         split_sentence = i.split(\",\")\n",
    "#         print(\"i ---\",split_sentence)\n",
    "\n",
    "#         unique_SA.add(sentences_dict[i])\n",
    "#         non_unique_SAs_per_group.append(sentences_dict[i])  #added 14feb\n",
    "#         country_set.add(split_sentence[0])\n",
    "#         age_set.add(split_sentence[1])\n",
    "#         alumni_of_set.add(split_sentence[2])\n",
    "#         lang_spoken_set.add(split_sentence[3])\n",
    "\n",
    "#       #### cluster size less than k\n",
    "#       if len(group) < k:\n",
    "#         print(\"less than k\")\n",
    "#         #### cluster size less than k/2, discard the group\n",
    "#         if len(group)< k/2:\n",
    "#           print(\"discarding group -- len<k/2\")      #################### Need to add new code for assigning these groups to valid groups here\n",
    "#           discarded_entries = discarded_entries + len(group)\n",
    "#         #### cluster size > k/2\n",
    "#         else:\n",
    "#           VtU_num = VtU_num + len(group)\n",
    "#           #### cluster size > k/2 and l-diversity not satisfied\n",
    "#           if len(unique_SA) < l:\n",
    "#             print(\"l-diversity not satisfied --- selecting random SA which will satisfy l-diversity\")\n",
    "\n",
    "#             fake_nodes_in_a_group = max( (l - len(unique_SA)) , (k -len(group)) )\n",
    "#             fake_nodes = fake_nodes + fake_nodes_in_a_group\n",
    "#             num_of_nodes_in_a_group = num_of_nodes_in_a_group + fake_nodes_in_a_group\n",
    "\n",
    "#             for i in range(0, fake_nodes_in_a_group):\n",
    "#               random_SA = random.choice(list(all_unique_SA))\n",
    "#               if random_SA in unique_SA:\n",
    "#                 random_SA = random.choice(list(all_unique_SA))\n",
    "#               print('random_SA:',random_SA)\n",
    "#               unique_SA.add(random_SA)\n",
    "#               non_unique_SAs_per_group.append(random_SA)  #added 14feb\n",
    "#               temp_group.append(group[0])  #added 14feb\n",
    "\n",
    "\n",
    "#             node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "#             node_val = unique_SA\n",
    "#             print(\"new l : \",unique_SA)\n",
    "#             invalid_to_valid_groups[node_key] = node_val\n",
    "#             valid_groups_with_count_of_nodes[node_key] = num_of_nodes_in_a_group\n",
    "\n",
    "#             countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "#             ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "#             alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "#             langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "\n",
    "#             #added 14feb\n",
    "#             new_SE_groups[new_index] = temp_group\n",
    "#             new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "#             new_index = new_index + 1\n",
    "\n",
    "\n",
    "#           #### cluster size > k/2 and l-diversity satisfied\n",
    "#           else:\n",
    "#             print(\"adding fake node\")\n",
    "#             fake_nodes_in_a_group = k -len(group)\n",
    "#             fake_nodes = fake_nodes + fake_nodes_in_a_group\n",
    "\n",
    "#             for i in range(0, fake_nodes_in_a_group):    #added 14feb\n",
    "#               random_SA = random.choice(list(all_unique_SA))\n",
    "#               if random_SA in unique_SA:\n",
    "#                 random_SA = random.choice(list(all_unique_SA))\n",
    "#               print('random_SA:',random_SA)\n",
    "#               unique_SA.add(random_SA)\n",
    "#               non_unique_SAs_per_group.append(random_SA)\n",
    "#               temp_group = group\n",
    "#               temp_group.append(group[0])  #added 14feb\n",
    "\n",
    "#             #added 14feb\n",
    "#             new_SE_groups[new_index] = temp_group\n",
    "#             new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "#             new_index = new_index + 1\n",
    "\n",
    "\n",
    "#             num_of_nodes_in_a_group = num_of_nodes_in_a_group + fake_nodes_in_a_group\n",
    "#             node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "#             node_val = unique_SA\n",
    "#             invalid_to_valid_groups[node_key] = node_val\n",
    "#             valid_groups_with_count_of_nodes[node_key] = num_of_nodes_in_a_group\n",
    "\n",
    "#             # countries_sum = countries_sum + num_of_nodes_in_a_group * len(country_set) -1\n",
    "#             # ages_sum = ages_sum +  num_of_nodes_in_a_group * len(age_set)-1\n",
    "#             # alumniOf_sum = alumniOf_sum + num_of_nodes_in_a_group* len(alumni_of_set)-1\n",
    "#             # langs_sum = langs_sum + num_of_nodes_in_a_group* len(lang_spoken_set)-1\n",
    "\n",
    "#             countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "#             ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "#             alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "#             langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "\n",
    "\n",
    "#       #### l-diversity not satisfied\n",
    "#       elif len(unique_SA) < l:\n",
    "#         print(\"l-diversity not satisfied --- selecting random SA which will satisfy l-diversity\")\n",
    "#         VtU_num = VtU_num + len(group)\n",
    "#         fake_nodes_in_a_group = l - len(unique_SA)\n",
    "#         fake_nodes = fake_nodes + fake_nodes_in_a_group\n",
    "#         num_of_nodes_in_a_group = num_of_nodes_in_a_group + fake_nodes_in_a_group\n",
    "\n",
    "#         for i in range(0, fake_nodes_in_a_group):\n",
    "#           random_SA = random.choice(list(all_unique_SA))\n",
    "#           if random_SA in unique_SA:\n",
    "#             random_SA = random.choice(list(all_unique_SA))\n",
    "#           print('random_SA:',random_SA)\n",
    "#           unique_SA.add(random_SA)\n",
    "#           non_unique_SAs_per_group.append(random_SA)  #added 14feb\n",
    "#           temp_group.append(group[0])  #added 14feb\n",
    "\n",
    "#         #added 14feb\n",
    "#         new_SE_groups[new_index] = temp_group\n",
    "#         new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "#         new_index = new_index + 1\n",
    "\n",
    "\n",
    "#         node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "#         node_val = unique_SA\n",
    "#         print(\"new l : \",unique_SA)\n",
    "#         invalid_to_valid_groups[node_key] = node_val\n",
    "#         valid_groups_with_count_of_nodes[node_key] = num_of_nodes_in_a_group\n",
    "\n",
    "#         countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "#         ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "#         alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "#         langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "\n",
    "#       #### cluster size more than 2k-1\n",
    "#       elif len(group) > (2*k-1):\n",
    "#         print(f\"length of the group {len(group)} is more than 2k-1 --- splitting the group\")\n",
    "#         num_of_groups = int(len(group) / k)\n",
    "#         print(\"num_of_groups : \",num_of_groups)\n",
    "#         subgroups = [group[i:i + (k+1)] for i in range(0, len(group), (k+1))]\n",
    "#         print(len(subgroups))\n",
    "#         groups.extend(subgroups)\n",
    "\n",
    "\n",
    "#       #### group satisfies k-anonimity and l-diversity\n",
    "#       else:\n",
    "#         print(\"in else\")\n",
    "#         # valid_groups.append(group)\n",
    "#         VtU_num = VtU_num + len(group)\n",
    "#         for i in group:\n",
    "#           print(i+\"---\"+sentences_dict[i])\n",
    "#           unique_SA.add(sentences_dict[i])\n",
    "\n",
    "#         node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "#         node_val = unique_SA\n",
    "#         invalid_to_valid_groups[node_key] = node_val\n",
    "#         valid_groups_with_count_of_nodes[node_key] = len(group)\n",
    "\n",
    "#         #added 14feb\n",
    "#         new_SE_groups[new_index] = temp_group\n",
    "#         new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "#         new_index = new_index + 1\n",
    "\n",
    "\n",
    "#         countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "#         ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "#         alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "#         langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "#       print(\"----------------------------------------\")\n",
    "\n",
    "#       all_countries = all_countries.union(country_set)\n",
    "#       all_ages = all_ages.union(age_set)\n",
    "#       all_alumniOfs = all_alumniOfs.union(alumni_of_set)\n",
    "#       all_langs = all_langs.union(lang_spoken_set)\n",
    "\n",
    "#     m = z\n",
    "\n",
    "#     small_SE_avgs = 0\n",
    "#     small_SA_avgs = 0\n",
    "#     SE_similarity = 0\n",
    "#     SA_similarity = 0\n",
    "\n",
    "#     # Create groups of three from each cluster\n",
    "#     # groups = [[] for _ in range(num_clusters)]\n",
    "#     # for i, sentence in enumerate(sentences):\n",
    "#     #     cluster_index = int(labels[i])\n",
    "#     #     groups[cluster_index].append(sentence)\n",
    "\n",
    "#     groups = list(new_SE_groups.values())\n",
    "\n",
    "#     valid_groups_count = 0\n",
    "\n",
    "\n",
    "#     for i, group in enumerate(groups):\n",
    "#           # print(group)\n",
    "#           small_SE_avgs = 0\n",
    "\n",
    "#           unique_SA = set()\n",
    "#         # if len(group) >= m:\n",
    "#           # print((group))\n",
    "#           # print('size:', len(group))\n",
    "#           SE_embeddings = model.encode(group)\n",
    "\n",
    "#           for sentence_index in range(len(group)):\n",
    "#             # print(sentence_index)\n",
    "#             # print(group[sentence_index])\n",
    "#             unique_SA.add(sentences_dict[group[sentence_index]])\n",
    "#             for sentence_index1 in range(sentence_index+1, len(group)):\n",
    "#               small_SE_avgs = small_SE_avgs + util.cos_sim(SE_embeddings[sentence_index], SE_embeddings[sentence_index1])\n",
    "#           # print(\"small_SE_avgs/comb(len(group), 2) :\",small_SE_avgs/comb(len(group), 2))\n",
    "\n",
    "\n",
    "#           SE_similarity = SE_similarity + small_SE_avgs/comb(len(group), 2)\n",
    "#           valid_groups_count = valid_groups_count + 1\n",
    "#           # print(unique_SA)\n",
    "\n",
    "#     groups1 = list(new_SA_groups.values())\n",
    "\n",
    "#     for i, group in enumerate(groups1):\n",
    "#           small_SA_avgs = 0\n",
    "#           SE_embeddings = model.encode(group)\n",
    "\n",
    "#           for sentence_index in range(len(group)):\n",
    "#             # print(sentence_index)\n",
    "#             # print(group[sentence_index])\n",
    "#             for sentence_index1 in range(sentence_index+1, len(group)):\n",
    "#               small_SA_avgs = small_SA_avgs + util.cos_sim(SE_embeddings[sentence_index], SE_embeddings[sentence_index1])\n",
    "#           SA_similarity = SA_similarity + small_SA_avgs/comb(len(group), 2)\n",
    "#           # print(\"small_SE_avgs/comb(len(group), 2) :\",small_SE_avgs/comb(len(group), 2))\n",
    "#           # unique_SA_list = list(unique_SA)\n",
    "#           # SA_embeddings2 = model.encode(unique_SA_list)\n",
    "#           # for sentence_index in range(len(unique_SA_list)):\n",
    "#           #   for sentence_index1 in range(sentence_index+1, len(unique_SA_list)):\n",
    "#           #     # print(sentence_index1)\n",
    "#           #     small_SA_avgs = small_SA_avgs + util.cos_sim(SA_embeddings2[sentence_index], SA_embeddings2[sentence_index1])\n",
    "#           # # print(unique_SA_list)\n",
    "#           # if len(unique_SA_list) >= l:\n",
    "#           # # print(small_SA_avgs/comb(len(unique_SA_list), 2))\n",
    "#           #   SA_similarity = SA_similarity + small_SA_avgs/comb(len(unique_SA_list), 2)\n",
    "\n",
    "\n",
    "#     print(SE_similarity)\n",
    "#     print(\"valid_groups_count : \", valid_groups_count)\n",
    "#     print(\"len(groups) : \", len(groups))\n",
    "#     final_SE_similarity = SE_similarity / valid_groups_count\n",
    "#     final_SA_similarity = SA_similarity / valid_groups_count\n",
    "#     print(\"final_SE_similarity : \",final_SE_similarity)\n",
    "#     print(\"final_SA_similarity : \",final_SA_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnOiG1aY_OC6",
    "outputId": "2689ea4b-efbe-4d41-d002-a4f38bb00150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  --  2\n",
      "tensor([[208.9444]])\n",
      "valid_groups_count :  253\n",
      "len(groups) :  253\n",
      "final_SE_similarity :  tensor([[0.8259]])\n",
      "final_SA_similarity :  tensor([[0.3658]])\n",
      "VtU_num : 808\n",
      "AAIL : 0.007563942192260139\n",
      "895\n",
      "AIL : 0.2772197377556941\n",
      "4  --  2\n",
      "tensor([[191.5948]])\n",
      "valid_groups_count :  230\n",
      "len(groups) :  230\n",
      "final_SE_similarity :  tensor([[0.8330]])\n",
      "final_SA_similarity :  tensor([[0.3512]])\n",
      "VtU_num : 887\n",
      "AAIL : 0.010242699951247046\n",
      "1044\n",
      "AIL : 0.2318824471808009\n",
      "5  --  2\n",
      "tensor([[130.5847]])\n",
      "valid_groups_count :  163\n",
      "len(groups) :  163\n",
      "final_SE_similarity :  tensor([[0.8011]])\n",
      "final_SA_similarity :  tensor([[0.3534]])\n",
      "VtU_num : 833\n",
      "AAIL : 0.01320168208036618\n",
      "927\n",
      "AIL : 0.25350269813694176\n",
      "6  --  2\n",
      "tensor([[118.8799]])\n",
      "valid_groups_count :  148\n",
      "len(groups) :  148\n",
      "final_SE_similarity :  tensor([[0.8032]])\n",
      "final_SA_similarity :  tensor([[0.3513]])\n",
      "VtU_num : 868\n",
      "AAIL : 0.016190987024994243\n",
      "994\n",
      "AIL : 0.236472612412168\n",
      "7  --  2\n",
      "tensor([[91.7478]])\n",
      "valid_groups_count :  118\n",
      "len(groups) :  118\n",
      "final_SE_similarity :  tensor([[0.7775]])\n",
      "final_SA_similarity :  tensor([[0.3586]])\n",
      "VtU_num : 837\n",
      "AAIL : 0.019655538184043843\n",
      "922\n",
      "AIL : 0.24669380201740207\n",
      "8  --  2\n",
      "tensor([[87.6437]])\n",
      "valid_groups_count :  112\n",
      "len(groups) :  112\n",
      "final_SE_similarity :  tensor([[0.7825]])\n",
      "final_SA_similarity :  tensor([[0.3478]])\n",
      "VtU_num : 863\n",
      "AAIL : 0.022423377290838497\n",
      "996\n",
      "AIL : 0.2533648339377446\n",
      "9  --  2\n",
      "tensor([[68.9249]])\n",
      "valid_groups_count :  90\n",
      "len(groups) :  90\n",
      "final_SE_similarity :  tensor([[0.7658]])\n",
      "final_SA_similarity :  tensor([[0.3474]])\n",
      "VtU_num : 826\n",
      "AAIL : 0.025761260733872103\n",
      "909\n",
      "AIL : 0.2654332248252787\n",
      "10  --  2\n",
      "tensor([[68.3890]])\n",
      "valid_groups_count :  90\n",
      "len(groups) :  90\n",
      "final_SE_similarity :  tensor([[0.7599]])\n",
      "final_SA_similarity :  tensor([[0.3450]])\n",
      "VtU_num : 864\n",
      "AAIL : 0.028392325234877645\n",
      "984\n",
      "AIL : 0.24749082215745355\n",
      "3  --  3\n",
      "tensor([[209.4186]])\n",
      "valid_groups_count :  253\n",
      "len(groups) :  253\n",
      "final_SE_similarity :  tensor([[0.8277]])\n",
      "final_SA_similarity :  tensor([[0.3526]])\n",
      "VtU_num : 808\n",
      "AAIL : 0.007693241506933185\n",
      "916\n",
      "AIL : 0.29390408202794976\n",
      "4  --  3\n",
      "tensor([[191.7082]])\n",
      "valid_groups_count :  230\n",
      "len(groups) :  230\n",
      "final_SE_similarity :  tensor([[0.8335]])\n",
      "final_SA_similarity :  tensor([[0.3498]])\n",
      "VtU_num : 887\n",
      "AAIL : 0.010287870537018948\n",
      "1050\n",
      "AIL : 0.23630984872984362\n",
      "5  --  3\n",
      "tensor([[130.5964]])\n",
      "valid_groups_count :  163\n",
      "len(groups) :  163\n",
      "final_SE_similarity :  tensor([[0.8012]])\n",
      "final_SA_similarity :  tensor([[0.3562]])\n",
      "VtU_num : 833\n",
      "AAIL : 0.013213698420381314\n",
      "928\n",
      "AIL : 0.2543178995519155\n",
      "6  --  3\n",
      "tensor([[118.8799]])\n",
      "valid_groups_count :  148\n",
      "len(groups) :  148\n",
      "final_SE_similarity :  tensor([[0.8032]])\n",
      "final_SA_similarity :  tensor([[0.3514]])\n",
      "VtU_num : 868\n",
      "AAIL : 0.016190987024994243\n",
      "994\n",
      "AIL : 0.236472612412168\n",
      "7  --  3\n",
      "tensor([[91.7478]])\n",
      "valid_groups_count :  118\n",
      "len(groups) :  118\n",
      "final_SE_similarity :  tensor([[0.7775]])\n",
      "final_SA_similarity :  tensor([[0.3520]])\n",
      "VtU_num : 837\n",
      "AAIL : 0.019655538184043843\n",
      "922\n",
      "AIL : 0.24669380201740207\n",
      "8  --  3\n",
      "tensor([[87.6437]])\n",
      "valid_groups_count :  112\n",
      "len(groups) :  112\n",
      "final_SE_similarity :  tensor([[0.7825]])\n",
      "final_SA_similarity :  tensor([[0.3464]])\n",
      "VtU_num : 863\n",
      "AAIL : 0.022423377290838497\n",
      "996\n",
      "AIL : 0.2533648339377446\n",
      "9  --  3\n",
      "tensor([[68.9249]])\n",
      "valid_groups_count :  90\n",
      "len(groups) :  90\n",
      "final_SE_similarity :  tensor([[0.7658]])\n",
      "final_SA_similarity :  tensor([[0.3511]])\n",
      "VtU_num : 826\n",
      "AAIL : 0.025761260733872103\n",
      "909\n",
      "AIL : 0.2654332248252787\n",
      "10  --  3\n",
      "tensor([[68.3890]])\n",
      "valid_groups_count :  90\n",
      "len(groups) :  90\n",
      "final_SE_similarity :  tensor([[0.7599]])\n",
      "final_SA_similarity :  tensor([[0.3427]])\n",
      "VtU_num : 864\n",
      "AAIL : 0.028392325234877645\n",
      "984\n",
      "AIL : 0.24749082215745355\n",
      "3  --  4\n",
      "tensor([[213.1101]])\n",
      "valid_groups_count :  252\n",
      "len(groups) :  252\n",
      "final_SE_similarity :  tensor([[0.8457]])\n",
      "final_SA_similarity :  tensor([[0.3251]])\n",
      "VtU_num : 808\n",
      "AAIL : 0.008890114800290965\n",
      "1077\n",
      "AIL : 0.4003558150033752\n",
      "4  --  4\n",
      "tensor([[192.0809]])\n",
      "valid_groups_count :  230\n",
      "len(groups) :  230\n",
      "final_SE_similarity :  tensor([[0.8351]])\n",
      "final_SA_similarity :  tensor([[0.3368]])\n",
      "VtU_num : 887\n",
      "AAIL : 0.010537489875079286\n",
      "1081\n",
      "AIL : 0.25841512813986617\n",
      "5  --  4\n",
      "tensor([[130.7179]])\n",
      "valid_groups_count :  163\n",
      "len(groups) :  163\n",
      "final_SE_similarity :  tensor([[0.8020]])\n",
      "final_SA_similarity :  tensor([[0.3488]])\n",
      "VtU_num : 833\n",
      "AAIL : 0.013321042102496536\n",
      "937\n",
      "AIL : 0.26157569698119487\n",
      "6  --  4\n",
      "tensor([[118.9035]])\n",
      "valid_groups_count :  148\n",
      "len(groups) :  148\n",
      "final_SE_similarity :  tensor([[0.8034]])\n",
      "final_SA_similarity :  tensor([[0.3498]])\n",
      "VtU_num : 868\n",
      "AAIL : 0.016224487510715455\n",
      "997\n",
      "AIL : 0.23879925291805518\n",
      "7  --  4\n",
      "tensor([[91.7635]])\n",
      "valid_groups_count :  118\n",
      "len(groups) :  118\n",
      "final_SE_similarity :  tensor([[0.7777]])\n",
      "final_SA_similarity :  tensor([[0.3528]])\n",
      "VtU_num : 837\n",
      "AAIL : 0.019689618118510416\n",
      "924\n",
      "AIL : 0.24835520602293637\n",
      "8  --  4\n",
      "tensor([[87.6437]])\n",
      "valid_groups_count :  112\n",
      "len(groups) :  112\n",
      "final_SE_similarity :  tensor([[0.7825]])\n",
      "final_SA_similarity :  tensor([[0.3455]])\n",
      "VtU_num : 863\n",
      "AAIL : 0.022423377290838497\n",
      "996\n",
      "AIL : 0.2533648339377446\n",
      "9  --  4\n",
      "tensor([[68.9249]])\n",
      "valid_groups_count :  90\n",
      "len(groups) :  90\n",
      "final_SE_similarity :  tensor([[0.7658]])\n",
      "final_SA_similarity :  tensor([[0.3471]])\n",
      "VtU_num : 826\n",
      "AAIL : 0.025761260733872103\n",
      "909\n",
      "AIL : 0.2654332248252787\n",
      "10  --  4\n",
      "tensor([[68.3890]])\n",
      "valid_groups_count :  90\n",
      "len(groups) :  90\n",
      "final_SE_similarity :  tensor([[0.7599]])\n",
      "final_SA_similarity :  tensor([[0.3436]])\n",
      "VtU_num : 864\n",
      "AAIL : 0.028392325234877645\n",
      "984\n",
      "AIL : 0.24749082215745355\n"
     ]
    }
   ],
   "source": [
    "all_z = [3,4,5,6,7,8,9,10]\n",
    "all_l = [2,3,4]\n",
    "\n",
    "# all_z = [3]\n",
    "# all_l = [2,3]\n",
    "for l in all_l:\n",
    "  for z in all_z:\n",
    "    print(z,\" -- \",l)\n",
    "    k = z\n",
    "    # Use PCA to reduce the dimensionality to 2 for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Use K-Medoids clustering\n",
    "    num_clusters = int(len(sentences_dict)/k)\n",
    "\n",
    "    kmedoids = KMedoids(n_clusters=num_clusters, random_state=42)\n",
    "    cluster_labels = kmedoids.fit_predict(embeddings)\n",
    "\n",
    "    # # Visualize clusters\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # for i in range(num_clusters):\n",
    "    #     cluster_points = reduced_embeddings[cluster_labels == i]\n",
    "    #     plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i + 1}', alpha=0.7)\n",
    "\n",
    "    # plt.title('Visualization of Sentence Embeddings with Clusters (PCA)')\n",
    "    # plt.xlabel('Principal Component 1')\n",
    "    # plt.ylabel('Principal Component 2')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # Create groups of three from each cluster\n",
    "    groups = [[] for _ in range(num_clusters)]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        cluster_index = cluster_labels[i]\n",
    "        groups[cluster_index].append(sentence)\n",
    "\n",
    "    # Display groups\n",
    "    #for i, group in enumerate(groups):\n",
    "        #print(f'Group {i + 1}:', group)\n",
    "        #print(f'Group {i + 1} size:', len(group))\n",
    "\n",
    "    valid_groups = []\n",
    "    invalid_groups = []\n",
    "    all_unique_SA = set()\n",
    "\n",
    "    for group in groups:\n",
    "      #print(len(group))\n",
    "      unique_SA = set()\n",
    "      for i in group:\n",
    "        #print(i)\n",
    "        unique_SA.add(sentences_dict[i])\n",
    "        all_unique_SA.add(sentences_dict[i])\n",
    "      #print(\"Unique l values : \",unique_SA)\n",
    "      if len(unique_SA) >= l and len(group) >= k and len(group) <= (2*k-1):\n",
    "        valid_groups.append(group)\n",
    "      else:\n",
    "        invalid_groups.append(group)\n",
    "      #print(\"------------------------------------------\")\n",
    "\n",
    "    #print(\"VALID GROUPS : \")\n",
    "    #for group in valid_groups:\n",
    "      #print(group)\n",
    "\n",
    "\n",
    "    #added 14feb\n",
    "    new_SE_groups = {}\n",
    "    new_SA_groups = {}\n",
    "    non_unique_SAs_per_group = []\n",
    "    new_index = 0\n",
    "    temp_group = []\n",
    "    #print(\"new_SE_groups: \",new_SE_groups)\n",
    "    #print(\"new_SE_groups: \",len(new_SE_groups))\n",
    "    #print(\"new_SA_groups: \",new_SA_groups)\n",
    "    #print(\"new_SA_groups: \",len(new_SA_groups))\n",
    "    #print(\"new_index: \",new_index)\n",
    "\n",
    "\n",
    "    #added 14feb\n",
    "    new_SE_groups = {}\n",
    "    new_SA_groups = {}\n",
    "    non_unique_SAs_per_group = []\n",
    "    new_index = 0\n",
    "    temp_group = []\n",
    "    #print(\"new_SE_groups: \",new_SE_groups)\n",
    "    #print(\"new_SE_groups: \",len(new_SE_groups))\n",
    "    #print(\"new_SA_groups: \",new_SA_groups)\n",
    "    #print(\"new_SA_groups: \",len(new_SA_groups))\n",
    "    #print(\"new_index: \",new_index)\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"INVALID GROUPS : \")\n",
    "    invalid_to_valid_groups = {}\n",
    "    valid_groups_with_count_of_nodes = {}\n",
    "    fake_nodes = 0\n",
    "    discarded_entries = 0\n",
    "\n",
    "    countries_sum = 0\n",
    "    ages_sum = 0\n",
    "    alumniOf_sum = 0\n",
    "    langs_sum = 0\n",
    "    VtU_num = 0\n",
    "\n",
    "    all_countries = set()\n",
    "    all_ages = set()\n",
    "    all_alumniOfs = set()\n",
    "    all_langs = set()\n",
    "\n",
    "    for group in groups:\n",
    "\n",
    "      num_of_nodes_in_a_group = len(group)\n",
    "      node_key = \"\"\n",
    "      node_val = \"\"\n",
    "      country_set = set()\n",
    "      age_set = set()\n",
    "      alumni_of_set = set()\n",
    "      lang_spoken_set = set()\n",
    "      unique_SA = set()\n",
    "      non_unique_SAs_per_group = []   #added 14feb\n",
    "      temp_group = []  #added 14feb\n",
    "      temp_group = group   #added 14feb\n",
    "      fake_nodes_in_a_group = 0\n",
    "\n",
    "      for i in group:\n",
    "        split_sentence = i.split(\",\")\n",
    "        #print(\"i ---\",split_sentence)\n",
    "\n",
    "        unique_SA.add(sentences_dict[i])\n",
    "        non_unique_SAs_per_group.append(sentences_dict[i])  #added 14feb\n",
    "        country_set.add(split_sentence[0])\n",
    "        age_set.add(split_sentence[1])\n",
    "        alumni_of_set.add(split_sentence[2])\n",
    "        lang_spoken_set.add(split_sentence[3])\n",
    "\n",
    "      #### cluster size less than k\n",
    "      if len(group) < k:\n",
    "        #print(\"less than k\")\n",
    "        #### cluster size less than k/2, discard the group\n",
    "        if len(group)< k/2:\n",
    "          #print(\"discarding group -- len<k/2\")      #################### Need to add new code for assigning these groups to valid groups here\n",
    "          discarded_entries = discarded_entries + len(group)\n",
    "        #### cluster size > k/2\n",
    "        else:\n",
    "          VtU_num = VtU_num + len(group)\n",
    "          #### cluster size > k/2 and l-diversity not satisfied\n",
    "          if len(unique_SA) < l:\n",
    "            #print(\"l-diversity not satisfied --- selecting random SA which will satisfy l-diversity\")\n",
    "\n",
    "            fake_nodes_in_a_group = max( (l - len(unique_SA)) , (k -len(group)) )\n",
    "            fake_nodes = fake_nodes + fake_nodes_in_a_group\n",
    "            num_of_nodes_in_a_group = num_of_nodes_in_a_group + fake_nodes_in_a_group\n",
    "\n",
    "            for i in range(0, fake_nodes_in_a_group):\n",
    "              random_SA = random.choice(list(all_unique_SA))\n",
    "              if random_SA in unique_SA:\n",
    "                random_SA = random.choice(list(all_unique_SA))\n",
    "              #print('random_SA:',random_SA)\n",
    "              unique_SA.add(random_SA)\n",
    "              non_unique_SAs_per_group.append(random_SA)  #added 14feb\n",
    "              temp_group.append(group[0])  #added 14feb\n",
    "\n",
    "\n",
    "            node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "            node_val = unique_SA\n",
    "            #print(\"new l : \",unique_SA)\n",
    "            invalid_to_valid_groups[node_key] = node_val\n",
    "            valid_groups_with_count_of_nodes[node_key] = num_of_nodes_in_a_group\n",
    "\n",
    "            countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "            ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "            alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "            langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "\n",
    "            #added 14feb\n",
    "            new_SE_groups[new_index] = temp_group\n",
    "            new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "            new_index = new_index + 1\n",
    "\n",
    "\n",
    "          #### cluster size > k/2 and l-diversity satisfied\n",
    "          else:\n",
    "            #print(\"adding fake node\")\n",
    "            fake_nodes_in_a_group = k -len(group)\n",
    "            fake_nodes = fake_nodes + fake_nodes_in_a_group\n",
    "\n",
    "            for i in range(0, fake_nodes_in_a_group):    #added 14feb\n",
    "              random_SA = random.choice(list(all_unique_SA))\n",
    "              if random_SA in unique_SA:\n",
    "                random_SA = random.choice(list(all_unique_SA))\n",
    "              #print('random_SA:',random_SA)\n",
    "              unique_SA.add(random_SA)\n",
    "              non_unique_SAs_per_group.append(random_SA)\n",
    "              temp_group = group\n",
    "              temp_group.append(group[0])  #added 14feb\n",
    "\n",
    "            #added 14feb\n",
    "            new_SE_groups[new_index] = temp_group\n",
    "            new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "            new_index = new_index + 1\n",
    "\n",
    "\n",
    "            num_of_nodes_in_a_group = num_of_nodes_in_a_group + fake_nodes_in_a_group\n",
    "            node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "            node_val = unique_SA\n",
    "            invalid_to_valid_groups[node_key] = node_val\n",
    "            valid_groups_with_count_of_nodes[node_key] = num_of_nodes_in_a_group\n",
    "\n",
    "            # countries_sum = countries_sum + num_of_nodes_in_a_group * len(country_set) -1\n",
    "            # ages_sum = ages_sum +  num_of_nodes_in_a_group * len(age_set)-1\n",
    "            # alumniOf_sum = alumniOf_sum + num_of_nodes_in_a_group* len(alumni_of_set)-1\n",
    "            # langs_sum = langs_sum + num_of_nodes_in_a_group* len(lang_spoken_set)-1\n",
    "\n",
    "            countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "            ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "            alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "            langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "\n",
    "\n",
    "      #### l-diversity not satisfied\n",
    "      elif len(unique_SA) < l:\n",
    "        #print(\"l-diversity not satisfied --- selecting random SA which will satisfy l-diversity\")\n",
    "        VtU_num = VtU_num + len(group)\n",
    "        fake_nodes_in_a_group = l - len(unique_SA)\n",
    "        fake_nodes = fake_nodes + fake_nodes_in_a_group\n",
    "        num_of_nodes_in_a_group = num_of_nodes_in_a_group + fake_nodes_in_a_group\n",
    "\n",
    "        for i in range(0, fake_nodes_in_a_group):\n",
    "          random_SA = random.choice(list(all_unique_SA))\n",
    "          if random_SA in unique_SA:\n",
    "            random_SA = random.choice(list(all_unique_SA))\n",
    "          #print('random_SA:',random_SA)\n",
    "          unique_SA.add(random_SA)\n",
    "          non_unique_SAs_per_group.append(random_SA)  #added 14feb\n",
    "          temp_group.append(group[0])  #added 14feb\n",
    "\n",
    "        #added 14feb\n",
    "        new_SE_groups[new_index] = temp_group\n",
    "        new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "        new_index = new_index + 1\n",
    "\n",
    "\n",
    "        node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "        node_val = unique_SA\n",
    "        #print(\"new l : \",unique_SA)\n",
    "        invalid_to_valid_groups[node_key] = node_val\n",
    "        valid_groups_with_count_of_nodes[node_key] = num_of_nodes_in_a_group\n",
    "\n",
    "        countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "        ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "        alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "        langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "\n",
    "      #### cluster size more than 2k-1\n",
    "      elif len(group) > (2*k-1):\n",
    "        #print(f\"length of the group {len(group)} is more than 2k-1 --- splitting the group\")\n",
    "        num_of_groups = int(len(group) / k)\n",
    "        #print(\"num_of_groups : \",num_of_groups)\n",
    "        subgroups = [group[i:i + (k+1)] for i in range(0, len(group), (k+1))]\n",
    "        #print(len(subgroups))\n",
    "        groups.extend(subgroups)\n",
    "\n",
    "\n",
    "      #### group satisfies k-anonimity and l-diversity\n",
    "      else:\n",
    "        #print(\"in else\")\n",
    "        # valid_groups.append(group)\n",
    "        VtU_num = VtU_num + len(group)\n",
    "        for i in group:\n",
    "          #print(i+\"---\"+sentences_dict[i])\n",
    "          unique_SA.add(sentences_dict[i])\n",
    "\n",
    "        node_key = f\" {country_set}, {age_set}, {alumni_of_set}, {lang_spoken_set}\"\n",
    "        node_val = unique_SA\n",
    "        invalid_to_valid_groups[node_key] = node_val\n",
    "        valid_groups_with_count_of_nodes[node_key] = len(group)\n",
    "\n",
    "        #added 14feb\n",
    "        new_SE_groups[new_index] = temp_group\n",
    "        new_SA_groups[new_index] = non_unique_SAs_per_group\n",
    "        new_index = new_index + 1\n",
    "\n",
    "\n",
    "        countries_sum = countries_sum + len(group) * (len(country_set) -1)\n",
    "        ages_sum = ages_sum +  len(group) * (len(age_set)-1)\n",
    "        alumniOf_sum = alumniOf_sum + len(group)* (len(alumni_of_set)-1)\n",
    "        langs_sum = langs_sum + len(group)* (len(lang_spoken_set)-1)\n",
    "      #print(\"----------------------------------------\")\n",
    "\n",
    "      all_countries = all_countries.union(country_set)\n",
    "      all_ages = all_ages.union(age_set)\n",
    "      all_alumniOfs = all_alumniOfs.union(alumni_of_set)\n",
    "      all_langs = all_langs.union(lang_spoken_set)\n",
    "\n",
    "    m = z\n",
    "\n",
    "    small_SE_avgs = 0\n",
    "    small_SA_avgs = 0\n",
    "    SE_similarity = 0\n",
    "    SA_similarity = 0\n",
    "\n",
    "    # Create groups of three from each cluster\n",
    "    # groups = [[] for _ in range(num_clusters)]\n",
    "    # for i, sentence in enumerate(sentences):\n",
    "    #     cluster_index = int(labels[i])\n",
    "    #     groups[cluster_index].append(sentence)\n",
    "\n",
    "    groups = list(new_SE_groups.values())\n",
    "\n",
    "    valid_groups_count = 0\n",
    "\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "          # print(group)\n",
    "          small_SE_avgs = 0\n",
    "\n",
    "          unique_SA = set()\n",
    "        # if len(group) >= m:\n",
    "          # print((group))\n",
    "          # print('size:', len(group))\n",
    "          SE_embeddings = model.encode(group)\n",
    "\n",
    "          for sentence_index in range(len(group)):\n",
    "            # print(sentence_index)\n",
    "            # print(group[sentence_index])\n",
    "            unique_SA.add(sentences_dict[group[sentence_index]])\n",
    "            for sentence_index1 in range(sentence_index+1, len(group)):\n",
    "              small_SE_avgs = small_SE_avgs + util.cos_sim(SE_embeddings[sentence_index], SE_embeddings[sentence_index1])\n",
    "          # print(\"small_SE_avgs/comb(len(group), 2) :\",small_SE_avgs/comb(len(group), 2))\n",
    "\n",
    "\n",
    "          SE_similarity = SE_similarity + small_SE_avgs/comb(len(group), 2)\n",
    "          valid_groups_count = valid_groups_count + 1\n",
    "          # print(unique_SA)\n",
    "\n",
    "    groups1 = list(new_SA_groups.values())\n",
    "\n",
    "    for i, group in enumerate(groups1):\n",
    "          small_SA_avgs = 0\n",
    "          SE_embeddings = model.encode(group)\n",
    "\n",
    "          for sentence_index in range(len(group)):\n",
    "            # print(sentence_index)\n",
    "            # print(group[sentence_index])\n",
    "            for sentence_index1 in range(sentence_index+1, len(group)):\n",
    "              small_SA_avgs = small_SA_avgs + util.cos_sim(SE_embeddings[sentence_index], SE_embeddings[sentence_index1])\n",
    "          SA_similarity = SA_similarity + small_SA_avgs/comb(len(group), 2)\n",
    "          # print(\"small_SE_avgs/comb(len(group), 2) :\",small_SE_avgs/comb(len(group), 2))\n",
    "          # unique_SA_list = list(unique_SA)\n",
    "          # SA_embeddings2 = model.encode(unique_SA_list)\n",
    "          # for sentence_index in range(len(unique_SA_list)):\n",
    "          #   for sentence_index1 in range(sentence_index+1, len(unique_SA_list)):\n",
    "          #     # print(sentence_index1)\n",
    "          #     small_SA_avgs = small_SA_avgs + util.cos_sim(SA_embeddings2[sentence_index], SA_embeddings2[sentence_index1])\n",
    "          # # print(unique_SA_list)\n",
    "          # if len(unique_SA_list) >= l:\n",
    "          # # print(small_SA_avgs/comb(len(unique_SA_list), 2))\n",
    "          #   SA_similarity = SA_similarity + small_SA_avgs/comb(len(unique_SA_list), 2)\n",
    "\n",
    "\n",
    "    print(SE_similarity)\n",
    "    print(\"valid_groups_count : \", valid_groups_count)\n",
    "    print(\"len(groups) : \", len(groups))\n",
    "    final_SE_similarity = SE_similarity / valid_groups_count\n",
    "    final_SA_similarity = SA_similarity / valid_groups_count\n",
    "    print(\"final_SE_similarity : \",final_SE_similarity)\n",
    "    print(\"final_SA_similarity : \",final_SA_similarity)\n",
    "\n",
    "    # number of users in anonymized KG\n",
    "    # VtU_num = sum(valid_groups_with_count_of_nodes.values())\n",
    "    print(\"VtU_num :\",VtU_num)\n",
    "\n",
    "    # number of user-attribute relationship types (country, age, alumniOf, languageKnown)\n",
    "    RtUA_num = 4\n",
    "\n",
    "    loss1 = (countries_sum/(len(all_countries)-1)) + (ages_sum/(len(all_ages)-1)) + (alumniOf_sum/(len(all_alumniOfs)-1)) + (langs_sum/(len(all_langs)-1))\n",
    "    AL = loss1 / RtUA_num\n",
    "    AAIL = AL / VtU_num\n",
    "    print(\"AAIL :\",AAIL)\n",
    "\n",
    "    #fake_nodes\n",
    "    #discarded_entries\n",
    "\n",
    "    # number of users in union of set of users in anonymized KG and original KG\n",
    "    Utu = len(sentences_dict) - discarded_entries + fake_nodes\n",
    "    print(Utu)\n",
    "\n",
    "    AIL = (AL + (fake_nodes+discarded_entries)) / Utu\n",
    "\n",
    "    print(\"AIL :\",AIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsziEfOZD-3L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
